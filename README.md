# ü§ñ GitHub-Chatbot-LLM

A **local RAG-based chatbot** for exploring GitHub repositories with **Ollama LLMs** (LLaMA 3, Mistral, etc.), **FAISS** embeddings, and a clean **React frontend** + **FastAPI backend**.

---

## ‚ú® Features
- üîç Retrieval-Augmented Generation (RAG) over cloned repositories 
- ‚ö° GPU-accelerated embeddings with Hugging Face BGE models 
- üñ•Ô∏è FastAPI backend + React (Vite) frontend 
- üíæ Persistent FAISS vectorstore for efficient queries 
- üõ†Ô∏è Easy setup with virtual environment & Ollama models 

---

## üì¶ Tech Stack
![Python](https://img.shields.io/badge/Python-3.13-blue?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green?logo=fastapi)
![React](https://img.shields.io/badge/React-18-61dafb?logo=react)
![Vite](https://img.shields.io/badge/Vite-5-purple?logo=vite)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLMs-black?logo=ollama)
![FAISS](https://img.shields.io/badge/FAISS-Vector_DB-orange?logo=databricks)

---

## üìÇ Project Structure
```
GitHub-Chatbot-LLM/
‚îú‚îÄ‚îÄ backend/
‚îÇ ‚îú‚îÄ‚îÄ api.py # FastAPI server
‚îÇ ‚îú‚îÄ‚îÄ chat/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ rag_chat.py # core RAG logic
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ history_utils.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ logging_config.py
‚îÇ ‚îú‚îÄ‚îÄ ingest/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ store_embeddings.py # embedding pipeline
‚îÇ ‚îú‚îÄ‚îÄ clone_dsa_repos.sh # repository cloning
‚îÇ ‚îú‚îÄ‚îÄ vectorstore/ # FAISS indices
‚îÇ ‚îî‚îÄ‚îÄ data/ # cloned repositories
‚îú‚îÄ‚îÄ frontend/
‚îÇ ‚îú‚îÄ‚îÄ src/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ App.jsx # main React app
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ main.jsx
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ App.css
‚îÇ ‚îú‚îÄ‚îÄ index.html
‚îÇ ‚îî‚îÄ‚îÄ vite.config.js
‚îî‚îÄ‚îÄ active.sh # environment setup
```

---

## ‚öôÔ∏è Setup Guide

### 1Ô∏è‚É£ Clone Required Repositories
```bash
cd backend
chmod +x clone_dsa_repos.sh
bash clone_dsa_repos.sh
```

### 2Ô∏è‚É£ Pull Ollama Model
```bash
ollama pull llama3:latest
```

### 3Ô∏è‚É£ Create & Activate Virtual Environment
```bash
cd /nashome/bhavesh/GitHub-Chatbot-LLM
python3.13 -m venv backend/.venv
source backend/.venv/bin/activate
pip install --upgrade pip
```

Install Torch (GPU-enabled):
```bash
pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124
```

Optional (for PDFs/Office/images):
```bash
pip install "unstructured[all-docs]"
```

Project dependencies:
```bash
pip install -r backend/requirements.txt
```

---

### 4Ô∏è‚É£ Build Embeddings

**Base model:**
```bash
python backend/ingest/store_embeddings.py backend/data/ --out backend/vectorstore --model BAAI/bge-base-en-v1.5 --device auto --batch-size 32
```

**Large model:**
```bash
python backend/ingest/store_embeddings.py backend/data/ --out backend/vectorstore --model BAAI/bge-large-en-v1.5 --device auto --chunk-size 1200 --overlap 200 --batch-size 64 --chunks-per-batch 8000
```

**Background run:**
```bash
nohup python backend/ingest/store_embeddings.py backend/data/ --out backend/vectorstore --model BAAI/bge-large-en-v1.5 --device auto --chunk-size 1200 --overlap 200 --batch-size 64 --chunks-per-batch 8000 > embeddings.log 2>&1 &
```

**Re-embedding multiple folders:**
```bash
nohup python backend/ingest/store_embeddings.py backend/data backend/data/otherfiles --out backend/vectorstore --model BAAI/bge-large-en-v1.5 --device cuda --chunk-size 1200 --overlap 200 --batch-size 128 --chunks-per-batch 16000 --workers 8 --lang en --rembed > embeddings.log 2>&1 &
```

üìå Output is saved to:
```
backend/vectorstore/
```

The model name is stored in:
```
backend/vectorstore/model.txt
```

---

### 5Ô∏è‚É£ Start Backend API

**Interactive run:**
```bash
uvicorn backend.api:app --host 0.0.0.0 --port 8000
```

**Background run:**
```bash
cd backend
nohup uvicorn backend.api:app --host 0.0.0.0 --port 8000 --reload > backend.log 2>&1 &
```

**Health check:**
```bash
curl http://localhost:8000/health
```

---

### 6Ô∏è‚É£ Launch Frontend
```bash
cd frontend
npm install
npm run dev
```

**Background run:**
```bash
nohup npm run dev > npm.log 2>&1 &
```

Open in browser:
```
http://<host>:5173
```

---

### 7Ô∏è‚É£ Quick API Test
```bash
curl -s http://localhost:8000/chat -H 'Content-Type: application/json' -d '{"question":"How does HistomicsUI talk to HistomicsTK?", "model":"llama3:latest"}' | jq
```

---

## ‚úÖ Done!
Your chatbot should now be running with both **backend** and **frontend** accessible.

---

## üìú License
Gutman-Lab, Department of Pathology & Laboratory Medicine, Emory University
